---
title: "p8105_hw3_stl2137.Rmd"
output: github_document
---

# Homework 3
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(viridis)
library(p8105.datasets)
data("instacart")
data("brfss_smart2010")
```

## Problem 1

The instacart dataset is comprised of `r nrow(instacart)` observations and `r ncol(instacart)` variables. Key variables (in the context of this analysis) include `product_name`, which is the name of the product (for example: 96% Lean Ground Beef, 24 Carrot Gold Paleo Muffins, 2nd Foods Bananas); `aisle`, which is the name of the aisle a product comes from (for example: meat counter, breakfast bakery, baby food formula); `order_dow`, which is the day of the week on which the order was placed; and `order_hour_of_day`, which is the hour of the day on which the order was placed.  

```{r}
aisle_count <- instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
There are a total of `r nrow(aisle_count)` aisles, and the top 6 aisles where the most items are ordered from are fresh vegetables, fresh fruits, packaged vegetables fruits, yogurt, packcaged cheese, and water seltzer sparkling water. 

### Plot that shows Number of Items Ordered per Aisle
```{r}
instacart %>% 
  select(aisle, department) %>% 
  count(aisle) %>% 
  arrange(desc(n)) %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_col() +
  labs(
    x = "Name of Aisle",
    y = "Number of Items Ordered",
    title = "Number of Items Ordered per Aisle"
  ) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

### Table showing the three most popular items in the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  arrange(desc(n)) %>% 
  top_n(n = 3)
```

### Table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(order_dow, product_name) %>%
  summarise(
    mean_time = mean(order_hour_of_day) 
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_time
  ) 

```

## Problem 2

### Data Cleaning!
```{r}
brfss_clean <- brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic %in% "Overall Health") %>% 
  mutate(
    response = ordered(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))
  ) 
```

Using this dataset, do or answer the following (commenting on the results of each):

### In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
brfss_clean %>% 
  filter(year %in% 2002) %>% 
  group_by(locationabbr) %>% 
  summarise(
    count = length(unique(locationdesc))
  ) %>% 
  filter(
    count >= 7
  )

brfss_clean %>% 
  filter(year %in% 2010) %>% 
  group_by(locationabbr) %>% 
  summarise(
    count = length(unique(locationdesc))
  ) %>% 
  filter(
    count >= 7
  )
```

Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r}
brfss_excellent <- brfss_clean %>%
  filter(response %in% "Excellent") %>% 
  group_by(year, locationabbr) %>%
  summarise(
    mean_data_value =  mean(data_value, na.rm = TRUE)
  ) %>% 
  select(year, locationabbr, mean_data_value)
  
ggplot(brfss_excellent, aes(x = year, y = mean_data_value, group = locationabbr, color = locationabbr)) +
  geom_line() 
```

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State

```{r}
brfss_ny <- brfss_clean %>% 
  filter(year %in% c(2006, 2010), 
         locationabbr %in% "NY",
        )   

ggplot(brfss_ny, aes(x = response, y = data_value, group = interaction(response, locationdesc), fill = locationdesc)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "bottom") +
  facet_grid(.~year) +
  scale_fill_viridis(discrete = TRUE)


```

## Problem 3

```{r}
accel_data <- read_csv("./hw3_data/accel_data.csv")

accel_data_clean <- accel_data %>% 
  janitor::clean_names() %>% 
  mutate(
    weekend = ifelse(day %in% c("Saturday", "Sunday"), 1, 0),
    day = as.factor(day)
  ) %>% 
  group_by(day_id) %>% 
  pivot_longer(
    cols = -c(week, day, day_id),
    names_to = "activity_minute",
    values_to = "activity_value"
  ) %>%
  mutate(
    minute = seq(n()),
    hour = minute/60
  )
```

Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?
```{r}
total_activity_table <- accel_data_clean %>% 
  group_by(day_id) %>% 
  summarise(
    total_activity = sum(activity_value),
    day = unique(day)
  )
```

Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r}
ggplot(accel_data_clean, aes(x = hour, y = activity_value, group = day_id, color = day)) +
  geom_line() +
  theme(legend.position = "bottom")
```

